<!DOCTYPE html>
<html>
<head>
  <title>Workshop on BigData</title>
  <meta charset="utf-8">
  <meta name="description" content="Workshop on BigData">
  <meta name="author" content="Dr.Muthukumaran">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="libraries/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->  <link rel=stylesheet href="./assets/css/ribbons.css"></link>

  
  <!-- Grab CDN jQuery, fall back to local if offline -->
  <script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
  <script>window.jQuery || document.write('<script src="libraries/widgets/quiz/js/jquery.js"><\/script>')</script> 
  <script data-main="libraries/frameworks/io2012/js/slides" 
    src="libraries/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
  

</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
        <slide class="title-slide segue nobackground">
  <hgroup class="auto-fadein">
    <h1>Workshop on BigData</h1>
    <h2>Focus on Hadoop Basics</h2>
    <p>Dr.Muthukumaran<br/>http://profbmuthu.github.io/dataStream</p>
  </hgroup>
  <article></article>  
</slide>
    

    <!-- SLIDES -->
    <slide class="" id="slide-1" style="background:;">
  <hgroup>
    <h2>What is distributed Systems?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Distributed systems are often complex pieces of software of which the components
are by definition dispersed across multiple machines.</p></li>
<li><p>It is a system  in which hardware or software components located at networked computers communicate and coordinate their actions only by passing messages.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-2" style="background:;">
  <hgroup>
    <h2>What is File system Namespace ?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Traditional local file systems support a persistent name space. Local file system
views devices as being locally attached, the devices are not shared, and hence
there is no need in the file system design to enforce device sharing semantics.
HDFS supports a traditional hierarchical file organization. </p></li>
<li><p>A user or an application can create directories and store files inside these directories. </p></li>
<li><p>The file system namespace hierarchy is similar to most other existing file systems; one can create and remove files, move a file from one directory to another, or rename a file.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-3" style="background:;">
  <hgroup>
    <h2>What is data ingress and egress in hadoop?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Moving data in and out of Hadoop, which I’ll refer to in this chapter as data ingress and egress, is the process by which data is transported from an external system into an internal system, and vice versa. Hadoop supports ingress and egress at a low level in HDFS and MapReduce. </p></li>
<li><p>Files can be moved in and out of HDFS, and data can be pulled from external data sources and pushed to external data sinks using MapReduce.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-4" style="background:;">
  <hgroup>
    <h2>What is Hadoop?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Hadoop is a platform that provides both distributed storage and computational capabilities. Hadoop was first conceived to fix a scalability issue that existed in Nutch, an open source crawler and search engine.</p></li>
<li><p>Hadoop proper, is a distributed master-slave architecture that consists of the Hadoop Distributed File System (HDFS) for storage and Map-Reduce for computational capabilities.</p></li>
<li><p>The Apache Software Foundation is where all Apache Hadoop development happens.
Administrators can download Hadoop directly from the project website at http://ha
doop.apache.org.</p></li>
</ul>

<p>-Hadoop was created by Doug Cutting, the creator of Apache Lucene, the widely used
text search library. Hadoop has its origins in Apache Nutch, an open source web search engine, itself a part of the Lucene project. Apache Hadoop is distributed as tarballs containing both source and binary artifacts.</p>

<p>-Hadoop is designed to abstract away much of the complexity of distributed processing.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-5" style="background:;">
  <hgroup>
    <h2>What is Cloudera?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Cloudera, a company that provides support, consulting, and management tools for
Hadoop, also has a distribution of software called Cloudera’s Distribution Including Apache Hadoop, or just CDH.</li>
</ul>

<p>-Cloudera starts with a stable Apache Hadoop release, puts it on a steady release cadence, backports critical fixes, provides packages for a number of different operating systems, and has a commercial-grade QA and testing process</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-6" style="background:;">
  <hgroup>
    <h2>What is HDFS?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>HDFS is the storage component of Hadoop. </p></li>
<li><p>It’s a distributed filesystem that’s modeled after the Google File System (GFS) paper. </p></li>
<li><p>HDFS is optimized for high throughput and works best when reading and writing large files (gigabytes and larger).  </p></li>
<li><p>To support this throughput HDFS leverages unusually large (for a filesystem) block sizes and data locality optimizations to reduce network input/output (I/O).</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-7" style="background:;">
  <hgroup>
    <h2>What are the Hadoop daemons?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Hadoop has the following daemons:</p></li>
<li><p>NameNode: Maintains the metadata for each file stored in the HDFS. Metadata includes the information about blocks comprising the file as well their locations on the DataNodes.</p></li>
<li><p>Secondary NameNode: This is not a backup NameNode. In fact, it is a poorly named component of the Hadoop platform. It performs some housekeeping functions for the NameNode.</p></li>
<li><p>DataNode: Stores the actual blocks of a file in the HDFS on its own local
disk.</p></li>
<li><p>JobTracker: One of the master components, it is responsible for managing the overall execution of a job. It performs functions such as scheduling child tasks (individual Mapper and Reducer) to individual nodes, keeping track of the health of each task and node, and even rescheduling failed tasks.</p></li>
<li><p>TaskTracker: Runs on individual DataNodes and is responsible for starting and managing individual Map/Reduce tasks. Communicates with the JobTracker.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-8" style="background:;">
  <hgroup>
    <h2>What is a disk block?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>A disk has a block size, which is the minimum amount of data that it can read or
write. </p></li>
<li><p>Filesystems for a single disk build on this by dealing with data in blocks,
which are an integral multiple of the disk block size. </p></li>
<li><p>Filesystem blocks are typically a few kilobytes in size, whereas disk blocks are normally 512 bytes. </p></li>
<li><p>This is generally transparent to the filesystem user who is simply reading or writing a file of whatever length. There are tools to perform filesystem maintenance, such as df and fsck, that operate on the filesystem block level. </p></li>
<li><p>HDFS, has the concept of a block, but it is a much larger unit—64 MB by default. </p></li>
<li><p>Like in a filesystem for a single disk, files in HDFS are broken into block-sized chunks, which are stored as independent units.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-9" style="background:;">
  <hgroup>
    <h2>Why HDFS blocks are large?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>HDFS blocks are large compared to disk blocks, and the reason is to minimize
the cost of seeks. </p></li>
<li><p>By making a block large enough, the time to transfer the data from the disk can be significantly longer than the time to seek to the start of the block. Thus the time to transfer a large file made of multiple blocks operates at the disk transfer rate.</p></li>
<li><p>Having a block abstraction for a distributed filesystem brings several benefits.
The first benefit is the most obvious: a file can be larger than any single disk
in the network. There’s nothing that requires the blocks from a file to be stored on the same disk, so they can take advantage of any of the disks in the cluster.
In fact, it would be possible, if unusual, to store a single file on an HDFS cluster whose blocks filled all the disks in the cluster.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-10" style="background:;">
  <hgroup>
    <h2>What is name node?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The Name Node is a centralized service in the cluster operating on a single node.</p></li>
<li><p>The Name Node manages the file system namespace, which is where file system
tree and the metadata for all the files and directories are maintained. </p></li>
<li><p>The Name Node is the arbitrator and repository for all HDFS metadata.</p></li>
<li><p>The Name Node maintains metadata about the size and location of blocks and their replicas.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-11" style="background:;">
  <hgroup>
    <h2>What is a Data Node?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>In HDFS, the daemon responsible for storing and retrieving block data is called
the datanode (DN). </p></li>
<li><p>The data nodes are responsible for serving read and write requests from clients and perform block operations upon instructions from name node. </p></li>
<li><p>A Data node normally has no knowledge about HDFS files. While starting up, it scans through the local file system and creates a list of HDFS data blocks corresponding to each of these local files and sends this report to the Name node.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-12" style="background:;">
  <hgroup>
    <h2>What is data node storage?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Each Data Node stores HDFS blocks on behalf of local or remote clients. Each block is saved as a separate file in the node’s local file system. Because the Data Node abstracts away details of the local storage arrangement, all nodes do not have to use the same local file system. </p></li>
<li><p>Blocks are created or destroyed on Data Nodes at the request of the Name Node, which validates and processes requests from clients. </p></li>
<li><p>Although the Name Node manages the namespace, clients communicate directly with Data Nodes in order to read or write data at the HDFS block level. </p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-13" style="background:;">
  <hgroup>
    <h2>What is map reduce?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>MapReduce is built on the proven concept of divide and conquer: it’s much faster to break a massive task into smaller chunks and process them in parallel.</p></li>
<li><p>In MapReduce, task-based programming logic is placed as close to the data as possible. This technique works very nicely with both structured and unstructured data.</p></li>
<li><p>MapReduce works by breaking the processing into two phases: the map phase and the
reduce phase. Each phase has key-value pairs as input and output, the types of which may be chosen by the programmer. The programmer also specifies two functions: the map function and the reduce function.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-14" style="background:;">
  <hgroup>
    <h2>What is a map reduce job?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>A MapReduce job is a unit of work that the client wants to be performed: it consists of the input data, the MapReduce program, and configuration
information. </p></li>
<li><p>Hadoop runs the job by dividing it into tasks, of which there are two types: map tasks and reduce tasks.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-15" style="background:;">
  <hgroup>
    <h2>How are the map reduce jobs controlled?</h2>
  </hgroup>
  <article data-timings="">
    <p>There are two types of nodes that control the job execution process: a jobtracker and a number of tasktrackers. The jobtracker coordinates all the jobs run on the system by scheduling tasks to run on tasktrackers. Tasktrackers run tasks and send progress reports to the jobtracker, which keeps a record of the overall progress of each job. If a task fails, the jobtracker can reschedule it on a different tasktracker.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-16" style="background:;">
  <hgroup>
    <h2>What are the nodes of mapreduce framework</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The MapReduce framework has two types of nodes, master node and slave node. </p></li>
<li><p>JobTracker is the daemon on a master node, and TaskTracker is the daemon on a slave node. </p></li>
<li><p>The master node is the manager node of MapReduce jobs. It splits a job into smaller tasks, which will be assigned by the JobTracker to TaskTrackers on slave nodes to run. When a slave node receives a task, its TaskTracker will fork a Java process to run the task. </p></li>
<li><p>The TaskTracker is also responsible for tracking and reporting the progress of individual tasks.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-17" style="background:;">
  <hgroup>
    <h2>What is a Job tracker ?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The jobtracker is the master process, responsible for accepting job submissions from clients, scheduling tasks to run on worker nodes, and providing administrative functions such as worker health and task progress monitoring to the cluster. </p></li>
<li><p>There is one jobtracker per MapReduce cluster and it usually runs on reliable hardware since a failure of the master will result in the failure of all running jobs.</p></li>
<li><p>In order to provide job and task level-status, counters, and progress quickly, the jobtracker keeps metadata information about the last 100 (by default) jobs executed on the cluster in RAM</p></li>
<li><p>Due to the way job data is retained in memory, jobtracker memory requirements can grow independent of cluster size.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-18" style="background:;">
  <hgroup>
    <h2>What is task tracker?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The second daemon, the tasktracker, accepts task assignments from the jobtracker,
instantiates the user code, executes those tasks locally, and reports progress back to the jobtracker periodically. There is always a single tasktracker on each worker node.</p></li>
<li><p>Both tasktrackers and datanodes run on the same machines, which makes each node
both a compute node and a storage node, respectively. </p></li>
<li><p>Each tasktracker is configured with a specific number of map and reduce task slots that indicate how many of each type of task it is capable of executing in parallel. </p></li>
<li><p>A task slot is exactly what it sounds like; it is an allocation of available resources on a worker node to which a task may be assigned, in which case it is executed. </p></li>
<li><p>A tasktracker executes some number of map tasks and reduce tasks in parallel, so there is concurrency both within a worker where many tasks run, and at the cluster level where many workers exist.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-19" style="background:;">
  <hgroup>
    <h2>What are haoop splits?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Hadoop divides the input to a MapReduce job into fixed-size pieces called input
splits, or just splits. Hadoop creates one map task for each split, which runs the userdefined map function for each record in the split.</p></li>
<li><p>Having many splits means the time taken to process each split is small compared to the time to process the whole input. So if we are processing the splits in parallel, the processing is better load-balanced if the splits are small, since a faster machine will be able to process proportionally more splits over the course of the job than a slower machine. </p></li>
<li><p>Even if the machines are identical, failed processes or other jobs running concurrently make load balancing desirable, and the quality of the load balancing increases as the splits become more fine-grained.</p></li>
<li><p>if splits are too small, then the overhead of managing the splits and
of map task creation begins to dominate the total job execution time. For most jobs, a good split size tends to be the size of an HDFS block, 64 MB by default</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-20" style="background:;">
  <hgroup>
    <h2>What is data locality optimization?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Hadoop does its best to run the map task on a node where the input data resides in HDFS. This is called the data locality optimization. </p></li>
<li><p>It should now be clear why the optimal split size is the same as the block size: it is the largest size of input that can be guaranteed to be stored on a single node. </p></li>
<li><p>If the split spanned two blocks, it would be unlikely that any HDFS node stored both blocks, so some of the split would have to be transferred across the network to the node running the map task, which is clearly less efficient than running the whole map task using local data.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-21" style="background:;">
  <hgroup>
    <h2>What is a federation?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Federation is a feature,  created to overcome the limitation that all filesystem metadata must fit in memory. It differs from namenode high availability in that rather than a single namespace being served from one of two possible namenodes, multiple namenodes each serve a different slice of a larger namespace. </p></li>
<li><p>It’s possible to enable either federation or high availability, or even both simultaneously. Sometimes, federation is used to provide a different level of service to a slice of a global namespace.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-22" style="background:;">
  <hgroup>
    <h2>What is hadoop hardware?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>The term commodity hardware is often used to describe Hadoop hardware requirements.</li>
</ul>

<p>-commodity refers to mid-level rack servers with dual sockets, as much error-correcting RAM as is affordable, and SATA drives optimized for RAID storage.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-23" style="background:;">
  <hgroup>
    <h2>What are site and default configuration xml files?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The site XML files (those with site in their filenames) will grow as you start customizing your Hadoop cluster, and it can quickly become challenging to keep track of what changes you’ve made, and how they relate to the default configuration values.</p></li>
<li><p>default files</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-24" style="background:;">
  <hgroup>
    <h2>What does core-site.xml refer to?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>core-site.xml Contains system-level Hadoop configuration items, such as the HDFS URL, the Hadoop temporary directory, and script locations for rack-aware Hadoop clusters. </p></li>
<li><p>Settings in this file override the settings in core-default.xml. The default settings can be seen at <a href="http://hadoop.apache.org/common/docs/r1.0.0/core-default.html">http://hadoop.apache.org/common/docs/r1.0.0/core-default.html</a>.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-25" style="background:;">
  <hgroup>
    <h2>What does hdfs-site.xml refer to?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>hdfs-site.xml Contains HDFS settings such as the default file replication count, the block size, and whether permissions are enforced. To view the default settings you can look at <a href="http://hadoop.apache.org/common/docs/r1.0.0/hdfs-default.html">http://hadoop.apache.org/common/docs/r1.0.0/hdfs-default.html</a>. </li>
<li>Settings in this file override the settings in hdfs-default.xml. </li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-26" style="background:;">
  <hgroup>
    <h2>What does mapred-site.xml refer to?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>mapred-site.xml HDFS settings such as the default number of reduce tasks, default min/max task memory sizes, and speculative execution are all set here. </p></li>
<li><p>To view the default settings you can look at <a href="http://hadoop.apache.org/common/docs/r1.0.0/mapreddefault.html">http://hadoop.apache.org/common/docs/r1.0.0/mapreddefault.html</a>. </p></li>
<li><p>Settings in this file override the settings in mapred-default.xml.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-27" style="background:;">
  <hgroup>
    <h2>What des the term masters refer to?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Contains a list of hosts that are Hadoop masters. This name is misleading and
should have been called secondary-masters. When you start Hadoop it’ll launch
NameNode and JobTracker on the local host from which you issued the start command,
and then SSH to all the nodes in this file to launch the SecondaryNameNode.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-28" style="background:;">
  <hgroup>
    <h2>What des the term slaves refer to?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>slaves Contains a list of hosts that are Hadoop slaves. When you start Hadoop it will SSH to each host in this file and launch the DataNode and TaskTracker daemons.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-29" style="background:;">
  <hgroup>
    <h2>Get in touch</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Reach out to your instructor</li>
</ul>

<li><p>Index page<a href="http://profbmuthu.github.com/dataStream/index.html"> Back to Index </a></td> 
</td></p></li>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-30" style="background:;">
  <article data-timings="">
    
  </article>
  <!-- Presenter Notes -->
</slide>

    <slide class="backdrop"></slide>
  </slides>
  <div class="pagination pagination-small" id='io2012-ptoc' style="display:none;">
    <ul>
      <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=1 title='What is distributed Systems?'>
         1
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=2 title='What is File system Namespace ?'>
         2
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=3 title='What is data ingress and egress in hadoop?'>
         3
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=4 title='What is Hadoop?'>
         4
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=5 title='What is Cloudera?'>
         5
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=6 title='What is HDFS?'>
         6
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=7 title='What are the Hadoop daemons?'>
         7
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=8 title='What is a disk block?'>
         8
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=9 title='Why HDFS blocks are large?'>
         9
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=10 title='What is name node?'>
         10
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=11 title='What is a Data Node?'>
         11
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=12 title='What is data node storage?'>
         12
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=13 title='What is map reduce?'>
         13
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=14 title='What is a map reduce job?'>
         14
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=15 title='How are the map reduce jobs controlled?'>
         15
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=16 title='What are the nodes of mapreduce framework'>
         16
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=17 title='What is a Job tracker ?'>
         17
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=18 title='What is task tracker?'>
         18
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=19 title='What are haoop splits?'>
         19
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=20 title='What is data locality optimization?'>
         20
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=21 title='What is a federation?'>
         21
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=22 title='What is hadoop hardware?'>
         22
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=23 title='What are site and default configuration xml files?'>
         23
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=24 title='What does core-site.xml refer to?'>
         24
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=25 title='What does hdfs-site.xml refer to?'>
         25
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=26 title='What does mapred-site.xml refer to?'>
         26
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=27 title='What des the term masters refer to?'>
         27
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=28 title='What des the term slaves refer to?'>
         28
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=29 title='Get in touch'>
         29
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=30 title='NA'>
         30
      </a>
    </li>
  </ul>
  </div>  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
  <!-- Load Javascripts for Widgets -->
  
  <!-- LOAD HIGHLIGHTER JS FILES -->
  <script src="libraries/highlighters/highlight.js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <!-- DONE LOADING HIGHLIGHTER JS FILES -->
   
  </html>
