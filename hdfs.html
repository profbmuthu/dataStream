<!DOCTYPE html>
<html>
<head>
  <title>BigData - Data Management</title>
  <meta charset="utf-8">
  <meta name="description" content="BigData - Data Management">
  <meta name="author" content="Dr.Muthukumaran">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="libraries/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->  <link rel=stylesheet href="./assets/css/ribbons.css"></link>

  
  <!-- Grab CDN jQuery, fall back to local if offline -->
  <script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
  <script>window.jQuery || document.write('<script src="libraries/widgets/quiz/js/jquery.js"><\/script>')</script> 
  <script data-main="libraries/frameworks/io2012/js/slides" 
    src="libraries/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
  

</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
        <slide class="title-slide segue nobackground">
  <hgroup class="auto-fadein">
    <h1>BigData - Data Management</h1>
    <h2>Focus on HDFS Basics</h2>
    <p>Dr.Muthukumaran<br/>http://profbmuthu.github.io/dataStream</p>
  </hgroup>
  <article></article>  
</slide>
    

    <!-- SLIDES -->
    <slide class="" id="slide-1" style="background:;">
  <hgroup>
    <h1>Let Us Discuss HDFS</h1>
  </hgroup>
  <article data-timings="">
    
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-2" style="background:;">
  <hgroup>
    <h2>What is distributed Systems?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>When a dataset outgrows the storage capacity of a single physical machine, it becomes necessary to partition it across a number of separate machines. </p></li>
<li><p>Filesystems that manage the storage across a network of machines are called distributed filesystems. </p></li>
<li><p>Since they are network based, all the complications of network programming kick in, thus making distributed filesystems more complex than regular disk filesystems.</p></li>
<li><p>Distributed systems are often complex pieces of software of which the components are by definition dispersed across multiple machines.</p></li>
<li><p>It is a system  in which hardware or software components located at networked computers communicate and coordinate their actions only by passing messages.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-3" style="background:;">
  <hgroup>
    <h2>What is a filesystem in Hadoop?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Hadoop has an abstract notion of filesystem, of which HDFS is just one implementation.</p></li>
<li><p>The Java abstract class org.apache.hadoop.fs.FileSystem represents a filesystem in Hadoop, and there are several concrete implementations</p></li>
<li><p>Some of the common filesystems in Hadoop include local file system, (fs.LocalFileSystem)
HDFS (hdfs.DistributedFileSystem), HFTP (hdfs.HftpFileSystem)  HSFTP (hdfs.HsftpFileSystem), WebHDFS (hdfs.web.WebHdfsFile System), HAR (fs.HarFileSystem), KFS (fs.kfs.KosmosFileSystem)
Distributed Raid (hdfs.DistributedRaidFileSystem), View (viewfs.ViewFileSystem)</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-4" style="background:;">
  <hgroup>
    <h2>What is HDFS?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>HDFS is the storage component of Hadoop. Itâ€™s a distributed filesystem thatâ€™s modeled after the Google File System (GFS) paper. </p></li>
<li><p>The Apache Hadoop project defines  HDFS as: &quot; the primary storage system used by Hadoop applications&quot;</p></li>
<li><p>HDFS is implemented as a block-structured filesystem.</p></li>
<li><p>HDFS is optimized for high throughput and works best when reading and writing large files (gigabytes and larger).  </p></li>
<li><p>To support this throughput HDFS leverages unusually large (for a filesystem) block sizes and data locality optimizations to reduce network input/output (I/O).</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-5" style="background:;">
  <hgroup>
    <h2>What  is Streaming data access?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>HDFS is built around the idea that the most efficient data processing pattern is a write-once, read-many-times pattern. </p></li>
<li><p>A dataset is typically generated or copied from source, and then various analyses are performed on that dataset over time.</p></li>
<li><p>Each analysis will involve a large proportion, if not all, of the dataset, so the time to read the whole dataset is more important than the latency in reading the first record.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-6" style="background:;">
  <hgroup>
    <h2>What is Commodity hardware ?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Hadoop doesn&#39;t require expensive, highly reliable hardware. </p></li>
<li><p>It&#39;s designed to run on clusters of commodity hardware (commonly available hardware that can be obtained from multiple vendors)3 for which the chance of node failure across the cluster is high, at least for large clusters. </p></li>
<li><p>HDFS is designed to carry on working without a noticeable interruption to the user in the face of such failure.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-7" style="background:;">
  <hgroup>
    <h2>What is the need for HDFS ?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>HDFS is a filesystem designed for storing very large files with streaming data access patterns, running on clusters of commodity hardware.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-8" style="background:;">
  <hgroup>
    <h2>What are HDFS architectural tradeoffs</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>A large number of additional decisions and trade-offs govern HDFS architecture and implementation, including the following:</p></li>
<li><p>HDFS is optimized to support high-streaming read performance, and this comes at the expense of random seek performance. This means that if an application is reading from HDFS, it should avoid (or at least minimize) the number of seeks. Sequential reads are the preferred way to access HDFS files.</p></li>
<li><p>HDFS supports only a limited set of operations on files - writes, deletes, appends, and reads, but not updates. It assumes that the data will be written to the HDFS once, and then read multiple times.</p></li>
<li><p>HDFS does not provide a mechanism for local caching of data. The overhead of caching is large enough that data should simply be re-read from the source, which is not a problem for applications that are mostly doing sequential reads of large-sized data files.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-9" style="background:;">
  <hgroup>
    <h2>What is a disk block? (1/2)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>A disk has a block size, which is the minimum amount of data that it can read or write. </p></li>
<li><p>Filesystems for a single disk build on this by dealing with data in blocks, which are an integral multiple of the disk block size. </p></li>
<li><p>Filesystem blocks are typically a few kilobytes in size, whereas disk blocks are normally 512 bytes. </p></li>
<li><p>This is generally transparent to the filesystem user who is simply reading or writing a file of whatever length. There are tools to perform filesystem maintenance, such as df and fsck, that operate on the filesystem block level. </p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-10" style="background:;">
  <hgroup>
    <h2>What is a disk block? (2/2)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>HDFS, has the concept of a block, but it is a much larger unitâ€”64 MB by default. </p></li>
<li><p>Like in a filesystem for a single disk, files in HDFS are broken into block-sized chunks, which are stored as independent units.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-11" style="background:;">
  <hgroup>
    <h2>Why HDFS blocks are large?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>HDFS blocks are large compared to disk blocks, and the reason is to minimize the cost of seeks. </p></li>
<li><p>By making a block large enough, the time to transfer the data from the disk can be significantly longer than the time to seek to the start of the block. Thus the time to transfer a large file made of multiple blocks operates at the disk transfer rate.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-12" style="background:;">
  <hgroup>
    <h2>Why are the benefits of DFS? (1/2)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Having a block abstraction for a distributed filesystem brings several benefits. The first benefit is the most obvious: a file can be larger than any single disk in the network. </p></li>
<li><p>There iâ€™s nothing that requires the blocks from a file to be stored on the same disk, so they can take advantage of any of the disks in the cluster.</p></li>
<li><p>Second: making the unit of abstraction a block rather than a file simplifies the storage
subsystem.  Simplicity is something to strive for in all systems, but it is especially
important for a distributed system in which the failure modes are so varied. </p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-13" style="background:;">
  <hgroup>
    <h2>Why are the benefits of DFS? (2/2)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Third: To insure against corrupted blocks and disk and machine failure, each block is replicated to a small number of physically separate machines (typically three). If a block becomes unavailable, a copy can be read from another location in a way that is transparent to the client. </p></li>
<li><p>A block that is no longer available due to corruption or machine failure can be replicated from its alternative locations to other live machines to bring the replication factor back to the normal level.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-14" style="background:;">
  <hgroup>
    <h2>What is HDFS data integrity? (1/2)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>HDFS transparently checksums all data written to it and by default verifies checksums when reading data. </p></li>
<li><p>A separate checksum is created for every io.bytes.per.checksum bytes of data. The default is 512 bytes, and because a CRC-32 checksum is 4 bytes long, the storage overhead is less than 1%.</p></li>
<li><p>Datanodes are responsible for verifying the data they receive before storing the data and its checksum. This applies to data that they receive from clients and from other datanodes during replication. A client writing data sends it to a pipeline of datanodes, and the last datanode in the pipeline verifies the checksum.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-15" style="background:;">
  <hgroup>
    <h2>What is HDFS data integrity? (2/2)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>If it detects an error, the client receives a ChecksumException, a subclass of IOException, which it should handle in an application-specific manner; for example, by retrying the operation.</p></li>
<li><p>Each datanode keeps a persistent log of checksum verifications, so it knows the last time each of its blocks was verified. When a client successfully verifies a block, it tells the datanode, which updates its log. Keeping statistics such as these is valuable in detecting bad disks</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-16" style="background:;">
  <hgroup>
    <h2>What are the Hadoop daemons?(1/2)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Hadoop has the following daemons:</p></li>
<li><p><strong>NameNode</strong>: Maintains the metadata for each file stored in the HDFS. Metadata includes the information about blocks comprising the file as well their locations on the DataNodes.</p></li>
<li><p><strong>Secondary NameNode</strong>: This is not a backup NameNode. In fact, it is a poorly named component of the Hadoop platform. It performs some housekeeping functions for the NameNode.</p></li>
<li><p>DataNode: Stores the actual blocks of a file in the HDFS on its own local
disk.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-17" style="background:;">
  <hgroup>
    <h2>What are the Hadoop daemons?(2/2)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>JobTracker: One of the master components, it is responsible for managing the overall execution of a job. It performs functions such as scheduling child tasks (individual Mapper and Reducer) to individual nodes, keeping track of the health of each task and node, and even rescheduling failed tasks.</p></li>
<li><p>TaskTracker: Runs on individual DataNodes and is responsible for starting and managing individual Map/Reduce tasks. Communicates with the JobTracker.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-18" style="background:;">
  <hgroup>
    <h2>What is name node?(1/2)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The Name Node is a centralized service in the cluster operating on a single node.</p></li>
<li><p>The Name Node manages the file system namespace. It maintains the filesystem tree and the metadata for all the files and directories in the tree. </p></li>
<li><p>This information is stored persistently on the local disk in the form of two files: the namespace image and the edit log. </p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-19" style="background:;">
  <hgroup>
    <h2>What is name node?(2/2)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The Name Node is the arbitrator and repository for all HDFS metadata.</p></li>
<li><p>The Name Node maintains metadata about the size and location of blocks and their replicas.</p></li>
<li><p>The namenode also knows the datanodes on which all the blocks for a given file are located; however, it does not store block locations persistently, because this information is reconstructed from datanodes when the system starts.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-20" style="background:;">
  <hgroup>
    <h2>What is a Data Node?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>In HDFS, the daemon responsible for storing and retrieving block data is called the datanode (DN). </p></li>
<li><p>Datanodes are the workhorses of the filesystem. They store and retrieve blocks when
they are told to (by clients or the namenode), and they report back to the namenode
periodically with lists of blocks that they are storing.</p></li>
<li><p>The data nodes are responsible for serving read and write requests from clients and perform block operations upon instructions from name node. </p></li>
<li><p>A Data node normally has no knowledge about HDFS files. While starting up, it scans through the local file system and creates a list of HDFS data blocks corresponding to each of these local files and sends this report to the Name node.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-21" style="background:;">
  <hgroup>
    <h2>What is a Job tracker ?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The jobtracker is the master process, responsible for accepting job submissions from clients, scheduling tasks to run on worker nodes, and providing administrative functions such as worker health and task progress monitoring to the cluster. </p></li>
<li><p>There is one jobtracker per MapReduce cluster and it usually runs on reliable hardware since a failure of the master will result in the failure of all running jobs.</p></li>
<li><p>In order to provide job and task level-status, counters, and progress quickly, the jobtracker keeps metadata information about the last 100 (by default) jobs executed on the cluster in RAM</p></li>
<li><p>Due to the way job data is retained in memory, jobtracker memory requirements can grow independent of cluster size.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-22" style="background:;">
  <hgroup>
    <h2>What is task tracker? (1/3)</h2>
  </hgroup>
  <article data-timings="">
    <p>The TaskTracker daemon, which runs on each compute node of the Hadoop cluster, </p>

<ul>
<li><p>The tasktracker daemon, accepts task assignments from the jobtracker, instantiates the user code, executes those tasks locally, and reports progress back to the jobtracker periodically. There is always a single tasktracker on each worker node.</p></li>
<li><p>Both tasktrackers and datanodes run on the same machines, which makes each node both a compute node and a storage node, respectively. </p></li>
<li><p>Each tasktracker is configured with a specific number of map and reduce task slots that indicate how many of each type of task it is capable of executing in parallel. They accept requests for individual tasks such as Map, Reduce, and Shuffle operations.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-23" style="background:;">
  <hgroup>
    <h2>What is task tracker? (2/3)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Each TaskTracker is configured with a set of slots that is usually set up as the total number of cores available on the machine. When a request is received (from the JobTracker) to launch a task, the TaskTracker initiates a new JVM for the task.</p></li>
<li><p>A task slot is exactly what it sounds like; it is an allocation of available resources on a worker node to which a task may be assigned, in which case it is executed. </p></li>
<li><p>A tasktracker executes some number of map tasks and reduce tasks in parallel, so there is concurrency both within a worker where many tasks run, and at the cluster level where many workers exist.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-24" style="background:;">
  <hgroup>
    <h2>What is task tracker? (3/3)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The TaskTracker is assigned a task depending on how many free slots it has (total number of tasks = actual tasks running). </p></li>
<li><p>The TaskTracker is responsible for sending heartbeat messages to the JobTracker. Apart from telling the JobTracker that it is healthy, these messages also tell the JobTracker about the number of available free slots.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-25" style="background:;">
  <hgroup>
    <h2>What does data node store?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Each Data Node stores HDFS blocks on behalf of local or remote clients. Each block is saved as a separate file in the nodeâ€™s local file system. Because the Data Node abstracts away details of the local storage arrangement, all nodes do not have to use the same local file system. </p></li>
<li><p>Blocks are created or destroyed on Data Nodes at the request of the Name Node, which validates and processes requests from clients. </p></li>
<li><p>Although the Name Node manages the namespace, clients communicate directly with Data Nodes in order to read or write data at the HDFS block level. </p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-26" style="background:;">
  <hgroup>
    <h2>How does name node store meta data?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Because of the relatively low amount of metadata per file (it only tracks filenames, permissions, and
the locations of each block), the NameNode stores all of the metadata in the main memory, thus
allowing for a fast random access. </p></li>
<li><p>The metadata storage is designed to be compact. As a result, a NameNode with 4 GB of RAM is capable of supporting a huge number of files and directories.</p></li>
</ul>

<hr>

<h2>What is a cluster ?</h2>

<ul>
<li><p>Cluster: is a group of linked resources, where each resource (Rj) has a computation unit and a data storage unit. </p></li>
<li><p>The computation unit consists of a set of slots, where each slot has a given execution rate. In most Hadoop systems, each CPU core is considered as one slot. Similarly, the data storage unit has a given capacity and data retrieval rate. </p></li>
<li><p>Data in the Hadoop system is organized into files, which are usually large. Each file is split into small pieces, which are called slices. Usually, all slices in a system have the same size.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-27" style="background:;">
  <hgroup>
    <h2>What is File system Namespace ?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Traditional local file systems support a persistent name space. Local file system views devices as being locally attached, the devices are not shared, and hence there is no need in the file system design to enforce device sharing semantics. HDFS supports a traditional hierarchical file organization. </p></li>
<li><p>A user or an application can create directories and store files inside these directories. </p></li>
<li><p>The file system namespace hierarchy is similar to most other existing file systems; one can create and remove files, move a file from one directory to another, or rename a file.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-28" style="background:;">
  <hgroup>
    <h2>What is a federation?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Federation is a feature,  created to overcome the limitation that all filesystem metadata must fit in memory. </li>
<li><p>It differs from namenode high availability in that rather than a single namespace being served from one of two possible namenodes, multiple namenodes each serve a different slice of a larger namespace. </p></li>
<li><p>Itâ€™s possible to enable either federation or high availability, or even both simultaneously. Sometimes, federation is used to provide a different level of service to a slice of a global namespace.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-29" style="background:;">
  <hgroup>
    <h2>What is HDFS Federation?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The namenode keeps a reference to every file and block in the filesystem in memory, which means that on very large clusters with many files, memory becomes the limiting factor for scaling</p></li>
<li><p>HDFS Federation, introduced in the 2.x release series, allows a cluster to scale by adding
namenodes, each of which manages a portion of the filesystem namespace</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-30" style="background:;">
  <hgroup>
    <h2>What are the benefits of federation?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>benefits of HDFS Federation:</p></li>
<li><p>Namespace scalability - HDFS cluster storage scales horizontally, but the namespace does not. Large deployments (or deployments using a lot of small files) benefit from scaling the namespace by adding more NameNodes to the cluster.</p></li>
<li><p>Performance - Filesystem operation throughput is limited by a single NameNode. Adding more NameNodes to the cluster scales the filesystem read/write operation&#39;s throughput.</p></li>
<li><p>isolation - A single NameNode offers no isolation in a multi-user environment. An experimental application can overload the NameNode and slow down production-critical applications. With multiple NameNodes, different categories of applications and users can be isolated to different namespaces.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-31" style="background:;">
  <hgroup>
    <h2>What is a block pool?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>A namespace operates on a set of blocks called as a block pool. </p></li>
<li><p>Although a pool is dedicated to a specific namespace, the actual data can be allocated on any of the DataNodes in the cluster. </p></li>
<li><p>Each block pool is managed independently, which allows a namespace to generate block IDs for new blocks without the need for coordination with the other namespaces. </p></li>
<li><p>The failure of a NameNode does not prevent the DataNode from serving other NameNodes in the cluster.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-32" style="background:;">
  <hgroup>
    <h2>What is a namespace volume ?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>A namespace and its block pool together are called a namespace volume. This is a self-contained
unit of management. </p></li>
<li><p>Under federation, each namenode manages a namespace volume, which is made up of the metadata for the namespace, and a block pool containing all the blocks for the files in the namespace. </p></li>
<li><p>Namespace volumes are independent of each other, which means namenodes do not communicate with one another, and furthermore the failure of one namenode does not affect the availability of the namespaces managed by other namenodes.</p></li>
<li><p>When a NameNode/namespace is deleted, the corresponding block pool at the
DataNodes is deleted. Each namespace volume is upgraded as a unit, during cluster upgrade.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-33" style="background:;">
  <hgroup>
    <h2>How do you access Federated HDFS cluster?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>A client accesses the filesystem on behalf of the user by communicating with the namenode and datanodes. </p></li>
<li><p>The client presents a filesystem interface similar to a Portable Operating System Interface (POSIX), so the user code does not need to know about the namenode and datanode to function.</p></li>
<li><p>To access a federated HDFS cluster, clients use client-side mount tables to map file paths to namenodes. </p></li>
<li><p>This is managed in configuration using ViewFileSystem and the viewfs:// URIs.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-34" style="background:;">
  <hgroup>
    <h2>What is HDFS high availability?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The combination of replicating namenode metadata on multiple filesystems and using the secondary namenode to create checkpoints protects against data loss, but it does not provide high-availability of the filesystem.</p></li>
<li><p>The 2.x release series of Hadoop remedies this situation by adding support for HDFS high-availability (HA). In this implementation there is a pair of namenodes in an activestandby
configuration. In the event of the failure of the active namenode, the standby takes over its duties to continue servicing client requests without a significant interruption.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-35" style="background:;">
  <hgroup>
    <h2>What is checkpoint?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The NameNode keeps an image of the entire file system namespace and file Blockmap in memory. </p></li>
<li><p>This key metadata item is designed to be compact, such that a NameNode with 4 GB of RAM is plenty to support a huge number of files and directories. </p></li>
<li><p>When the NameNode starts up, it reads the FsImage and EditLog from disk, applies all the transactions from the EditLog to the in-memory representation of the FsImage, and flushes out this new version into a new FsImage on disk. </p></li>
<li><p>It can then truncate the old EditLog because its transactions have been applied to the persistent FsImage. This process is called a checkpoint.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-36" style="background:;">
  <hgroup>
    <h2>What is checkpoint?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The Checkpoint node uses parameter fs.checkpoint.period to check the interval between two consecutive checkpoints. </p></li>
<li><p>The Interval time is in seconds (default is 3600 second). </p></li>
<li><p>The Edit log file size is specified by parameter fs.checkpoint.size (default size 64MB) and a checkpoint triggers if size exceeds. </p></li>
<li><p>Multiple checkpoint nodes may be specified in the cluster configuration file.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-37" style="background:;">
  <hgroup>
    <h2>What are the architectural changes in 2.X for High availability/</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>architectural changes are needed to allow this to happen:</p></li>
<li><p>The namenodes must use highly available shared storage to share the edit log. When a standby namenode comes up, it reads up to the end of the shared edit log to synchronize its state with the active namenode, and then continues to read new entries as they are written by the active namenode.</p></li>
<li><p>Datanodes must send block reports to both namenodes because the block mappings are stored in a namenode&#39;s memory, and not on disk.</p></li>
<li><p>Clients must be configured to handle namenode failover, using a mechanism that is transparent to users.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-38" style="background:;">
  <hgroup>
    <h2>What is failover controller?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The transition from the active namenode to the standby is managed by a new entity in the system called the failover controller. </p></li>
<li><p>Failover controllers are pluggable, but the first implementation uses ZooKeeper to ensure that only one namenode is active. </p></li>
<li><p>Each namenode runs a lightweight failover controller process whose job it is to monitor its namenode for failures (using a simple heartbeating mechanism) and trigger a failover should a namenode fail.</p></li>
<li><p>Failover may also be initiated manually by an administrator, for example, in the case of routine maintenance. This is known as a graceful failover, since the failover controller arranges an orderly transition for both namenodes to switch roles.</p></li>
<li><p>In the case of an ungraceful failover, however, it is impossible to be sure that the failed namenode has stopped running. </p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-39" style="background:;">
  <hgroup>
    <h2>What is fencing?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The HA implementation goes to great lengths to ensure that the previously active namenode is prevented from doing any damage and causing corruption-a method known as fencing.</p></li>
<li><p>The system employs a range of fencing mechanisms, including killing the namenode&#39;s process, revoking its access to the shared storage directory (typically by using a vendor-specific NFS command), and disabling its network port via a remote management command. </p></li>
<li><p>As a last resort, the previously active namenode can be fenced with a technique rather graphically known as STONITH, or &quot;shoot the other node in the head,&quot; which uses a specialized
power distribution unit to forcibly power down the host machine  </p></li>
</ul>

<hr>

<h2>What are the choices for HA shared storage?</h2>

<ul>
<li><p>There are two choices for the highly available shared storage: an NFS filer, or a quorum
journal manager (QJM). </p></li>
<li><p>The QJM is a dedicated HDFS implementation, designed for the sole purpose of providing a highly available edit log, and is the recommended choice for most HDFS installations. The QJM runs as a group of journal nodes, and each edit must be written to a majority of the journal nodes. Typically, there are three journal
nodes, so the system can tolerate the loss of one of them. This arrangement is similar to the way ZooKeeper works, although it is important to realize that the QJM implementation does not use ZooKeeper.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-40" style="background:;">
  <hgroup>
    <h2>How is the file stored in Data Node?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>A file is a collection of data in Hadoop, which  can be made up of several blocks.  </p></li>
<li><p>The DataNode stores each HDFS data block in a separate file on its local filesystem with no
knowledge about the HDFS files themselves.  </p></li>
<li><p>They are stored on different DataNodes (individual machines in the cluster) chosen randomly on a block-by-block basis. </p></li>
<li><p>To improve throughput even further, the DataNode does not create all files in the same directory. Instead, it uses heuristics to determine the optimal number of files per directory, and creates subdirectories appropriately.</p></li>
<li><p>As a result, access to a file usually requires access to multiple DataNodes, which means that HDFS supports file sizes far larger than a single-machine disk capacity.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-41" style="background:;">
  <hgroup>
    <h2>What are file permissions in HDFS? (1/2)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>HDFS has a permissions model for files and directories that is much like POSIX.</p></li>
<li><p>There are three types of permission: the read permission (r), the write permission (w),
and the execute permission (x). </p></li>
<li><p>The read permission is required to read files or list the contents of a directory. </p></li>
<li><p>The write permission is required to write a file, or for a directory, to create or delete files or directories in it. </p></li>
<li><p>The execute permission is ignored for a file because you can&#39;t execute a file on HDFS, and for a directory this permission is required to access its children.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-42" style="background:;">
  <hgroup>
    <h2>What are file permissions in HDFS? (2/2) contd..</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Each file and directory has an owner, a group, and a mode. </p></li>
<li><p>The mode is made up of the permissions for the user who is the owner, the permissions for the users who are members of the group, and the permissions for users who are neither the owners nor
members of the group.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-43" style="background:;">
  <hgroup>
    <h2>What are the data formats in HDFS?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Data formats in HDFS are classified into splittable and unsplittable formats. </p></li>
<li><p>A splittable format is one in which a file can be reliably split into multiple pieces called splits at record boundaries. </p></li>
<li><p>A splittable file format can seek to the start of a record from any point. </p></li>
<li><p>Splittable file formats are MapReduce-friendly, since MapReduce splits files to read data from a file in parallel from different mappers.</p></li>
<li><p>A sequence file is a splittable file format that is typically used with MapReduce jobs. It is represented as a list of keys and values, each of which is an instance of a Writable, which basically represents a serializable class.</p></li>
<li><p>There are compression formats that are splittable, like bz2, preprocessed LZO, etc</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-44" style="background:;">
  <hgroup>
    <h2>How does a client access the filesystem?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>A client accesses the filesystem on behalf of the user by communicating with the namenode
and datanodes. </p></li>
<li><p>The client presents a filesystem interface similar to a Portable Operating System Interface (POSIX), so the user code does not need to know about the namenode and datanodes to function</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-45" style="background:;">
  <hgroup>
    <h2>How can HDFS be accessed?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Access to HDFS is through an instance of the FileSystem object. </p></li>
<li><p>A FileSystem class is an abstract base class for a generic filesystem. (In addition to HDFS, Apache provides implementation of FileSystem objects for other filesystems, including KosmosFileSystem, NativeS3FileSystem,
RawLocalFileSystem, and S3FileSystem.) </p></li>
<li><p>It may be implemented as a distributed filesystem, or as a &quot;local&quot; one that uses the locally connected disk. The local version exists for small Hadoop instances and for testing</p></li>
<li><p>HDFS can be accessed in the following different ways:</p></li>
<li><p>Java APIs</p></li>
<li><p>Hadoop command line APIs (FS shell)</p></li>
<li><p>C/C++ language wrapper APIs</p></li>
<li><p>WebDAV (work in progress)</p></li>
<li><p>DFSAdmin (command set for administration)</p></li>
<li><p>RESTful APIs for HDFS</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-46" style="background:;">
  <hgroup>
    <h2>What are Hadoop interfaces?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Hadoop is written in Java, and all Hadoop filesystem interactions are mediated through
the Java API.</p></li>
<li><p>There are two ways of accessing HDFS over HTTP: directly, where the HDFS daemons serve HTTP requests to clients; and via a proxy (or proxies), which accesses HDFS on the client&#39;s behalf using the usual DistributedFileSystem API.</p></li>
<li><p>WebHDFS implementation supports all filesystem operations, including Kerberos authentication.
WebHDFS must be enabled by setting dfs.webhdfs.enabled to true, which allows you to use webhdfs URIs.  The HTTP REST API that WebHDFS exposes is formally defined in a specification, so it is expected that over time clients in languages other than Java will be written that use it directly.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-47" style="background:;">
  <hgroup>
    <h2>What are Hadoop interfaces? contd..</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Hadoop provides a C library called libhdfs that mirrors the Java FileSystem interface.  It works using the Java Native Interface (JNI) to call a Java filesystem client. Hadoop comes with prebuilt libhdfs binaries for 32-bit Linux, but for other platforms, you will need to build them yourself using the instructions at <a href="http://wiki.apache.org/hadoop/LibHDFS">http://wiki.apache.org/hadoop/LibHDFS</a>.</p></li>
<li><p>Filesystem in Userspace (FUSE) allows filesystems that are implemented in user space to be integrated as a Unix filesystem. Fuse-DFS is implemented in C using libhdfs as the interface to HDFS.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-48" style="background:;">
  <hgroup>
    <h2>What is data ingress and egress in hadoop?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Moving data in and out of Hadoop,  is referred as data ingress and egress, </p></li>
<li><p>It is the process by which data is transported from an external system into an internal system, and vice versa. Hadoop supports ingress and egress at a low level in HDFS and MapReduce. </p></li>
<li><p>Files can be moved in and out of HDFS, and data can be pulled from external data sources and pushed to external data sinks using MapReduce.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-49" style="background:;">
  <hgroup>
    <h2>What are Hadoop specific file types ?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>HDFS also introduced several specialized files types that provide much richer functionality, which often simplifies data processing.</p></li>
<li><p>SequenceFile, </p></li>
<li><p>MapFile, </p></li>
<li><p>SetFile, </p></li>
<li><p>ArrayFile, and </p></li>
<li><p>BloomMapFile </p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-50" style="background:;">
  <hgroup>
    <h2>What is a sequentialFile ?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>SequenceFile provides a persistent data structure for binary key/value pairs. Here, different
instances of both key and value must represent the same Java class, but can have different sizes.
Similar to other Hadoop files, SequenceFiles are append-only.</p></li>
<li><p>SequenceFile has three available formats: Uncompressed, Record-Compressed, and Block-Compressed. The first two are stored in a record-based format, whereas the third one uses block-based format</p></li>
<li><p>The choice of a specific format for a sequence file defines the length of the file on the hard drive.
Block-Compressed files typically are the smallest, while Uncompressed are the largest.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-51" style="background:;">
  <hgroup>
    <h2>What is the structure of sequence file? (1/2)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>A sequence file consists of a header followed by one or more records. The first three bytes of a sequence file are the bytes SEQ, which act as a magic number; these are followed by a single byte representing the version number. </p></li>
<li><p>The header contains other fields, including the names of the key and value classes, compression details, userdefined metadata, and the sync marker.</p></li>
<li><p>Recall that the sync marker is used to allow a reader to synchronize to a record boundary from any position in the file. Each file has a randomly generated sync marker, whose value is stored in the header. </p></li>
<li><p>Sync markers appear between records in the sequence file. They are designed to incur less than a 1%
storage overhead, so they don&#39;t necessarily appear between every pair of records.</p></li>
<li><p>The internal format of the records depends on whether compression is enabled, and if it is, whether it is record compression or block compression. </p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-52" style="background:;">
  <hgroup>
    <h2>What is the structure of sequence file? (2/2)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>If no compression is enabled (the default), each record is made up of the record length (in bytes), the key length, the key, and then the value. </p></li>
<li><p>The length fields are written as 4-byte integers adhering to the contract of the writeInt() method of java.io.DataOut put. </p></li>
<li><p>Keys and values are serialized using the Serialization defined for the class being written to the sequence file.</p></li>
<li><p>The format for record compression is almost identical to that for no compression, except the value bytes are compressed using the codec defined in the header</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-53" style="background:;">
  <article data-timings="">
    
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-54" style="background:;">
  <hgroup>
    <h2>What is a MapFile</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>A MapFile is a sorted SequenceFile with an index to permit lookups by key. </p></li>
<li><p>The index is itself a SequenceFile that contains a fraction of the keys in the map (every 128th key,
by default). </p></li>
</ul>

<p>The idea is that the index can be loaded into memory to provide fast lookups from the main data file, which is another SequenceFile containing all the map entries in sorted key order.</p>

<ul>
<li>MapFile offers a very similar interface to SequenceFile for reading and writing-the main thing to be aware of is that when writing using MapFile.Writer, map entries must be added in order, otherwise an IOException will be thrown.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-55" style="background:;">
  <hgroup>
    <h2>What is Map File varients?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Hadoop comes with a few variants on the general key-value MapFile interface:</p></li>
<li><p>SetFile is a specialization of MapFile for storing a set of Writable keys. The keys must be added in sorted order.</p></li>
<li><p>ArrayFile is a MapFile where the key is an integer representing the index of the
element in the array and the value is a Writable value.</p></li>
<li><p>BloomMapFile is a MapFile that offers a fast version of the get() method, especially for sparsely populated files. The implementation uses a dynamic Bloom filter for testing whether a given key is in the map. The test is very fast because it is inmemory, and it has a nonzero probability of false positives. Only if the test passes (the key is present) is the regular get() method called</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-56" style="background:;">
  <hgroup>
    <h2>What are row oriented file formats?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Sequence files, map files, and Avro datafiles are all row-oriented file formats, which means that the values for each row are stored contiguously in the file.</p></li>
<li><p>Roworiented formats are appropriate when a large number of columns of a single row are needed for processing at the same time.</p></li>
<li><p>Row-oriented formats like sequence files and Avro datafiles can be read up to the last sync point after a writer failure. It is for this reason that Flume uses row-oriented formats.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-57" style="background:;">
  <hgroup>
    <h2>What are column oriented file formats?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>In a columnoriented format, the rows in a file (or, equivalently, a table in Hive) are broken up into row splits, then each split is stored in column-oriented fashion: the values for each row in the first column are stored first, followed by the values for each row in the second column, and so on.</p></li>
<li><p>column-oriented formats work well when queries access only a small number of columns in the table.</p></li>
<li><p>Column-oriented formats need more memory for reading and writing, since they have to buffer a row split in memory, rather than just a single row.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-58" style="background:;">
  <hgroup>
    <h2>What is file compression?</h2>
  </hgroup>
  <article data-timings="">
    <p>An important consideration for storing data in HDFS files is data compression, shifting the computation load in data processing from I/O to CPU.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-59" style="background:;">
  <hgroup>
    <h2>What is file compression in Hadoop?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>File compression brings two major benefits: it reduces the space needed to store files, and it speeds up data transfer across the network or to or from disk. </p></li>
<li><p>When dealing with large volumes of data, both of these savings can be significant, so it pays to carefully consider how to use compression in Hadoop.</p></li>
<li><p>All compression algorithms exhibit a space/time trade-off: faster compression and decompression speeds usually come at the expense of smaller space savings.</p></li>
<li><p>A codec is the implementation of a compression-decompression algorithm. In Hadoop, a codec is represented by an implementation of the CompressionCodec interface.</p></li>
<li><p>CompressionCodecFactory finds codecs from a list defined by the io.compression. codecs configuration property</p></li>
<li><p>Each codec knows its default filename extension, thus permitting CompressionCodecFactory to search through
the registered codecs to find a match for a given extension</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-60" style="background:;">
  <hgroup>
    <h2>What are file compression tools in Hadoop?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Gzip is a generalpurpose compressor and sits in the middle of the space/time trade-off.</p></li>
<li><p>Bzip2 compresses more effectively than gzip, but is slower. Bzip2&#39;s decompression speed is faster than its compression speed, but it is still slower than the other formats.</p></li>
<li><p>LZO, LZ4. and Snappy, on the other hand, all optimize for speed and are around an order of magnitude faster than gzip, but compress less effectively.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-61" style="background:;">
  <hgroup>
    <h2>What are the methods of compression codec?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>CompressionCodec has two methods that allow you to easily compress or decompress data. </p></li>
<li><p>To compress data being written to an output stream, use the createOutput Stream(OutputStream out) method to create a CompressionOutputStream to which you write your uncompressed data to have it written in compressed form to the underlying stream. </p></li>
<li><p>Conversely, to decompress data being read from an input stream, call createInputStream(InputStream in) to obtain a CompressionInputStream, which allows you to read uncompressed data from the underlying stream</p></li>
<li><p>CompressionOutputStream and CompressionInputStream are similar to java.util.zip.DeflaterOutputStream and java.util.zip.DeflaterInputStream, except that both of the former provide the ability to reset their underlying compressor or decompressor, which is important for applications that compress sections of the data stream as separate blocks,</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-62" style="background:;">
  <hgroup>
    <h2>What is MapReduce?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Hadoop&#39;s main processing engine is MapReduce, which is currently one of the most popular big-data processing frameworks available. </p></li>
<li><p>It enables you to seamlessly integrate existing Hadoop data storage into processing, and it provides a unique combination of simplicity and power.</p></li>
<li><p>MapReduce is built on the proven concept of divide and conquer: itâ€™s much faster to break a massive task into smaller chunks and process them in parallel.</p></li>
<li><p>In MapReduce, task-based programming logic is placed as close to the data as possible. This technique works very nicely with both structured and unstructured data.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-63" style="background:;">
  <hgroup>
    <h2>What is the responsibility of MapReduce framework?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>MapReduce framework (based on the user-supplied code) is to provide the overall coordination of execution. </p></li>
<li><p>This includes choosing appropriate machines (nodes) for running mappers; starting and monitoring the mapper&#39;s execution; choosing appropriate locations for the reducer&#39;s execution; sorting and shuffling output of mappers and delivering the output to reducer nodes; and starting and monitoring the reducer&#39;s execution.</p></li>
<li><p>The framework takes care of scheduling tasks, monitoring them, and re-executing failed tasks.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-64" style="background:;">
  <hgroup>
    <h2>What are haoop splits?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Hadoop divides the input to a MapReduce job into fixed-size pieces called input splits, or just splits. Hadoop creates one map task for each split, which runs the userdefined map function for each record in the split.</p></li>
<li><p>Having many splits means the time taken to process each split is small compared to the time to process the whole input. So if we are processing the splits in parallel, the processing is better load-balanced if the splits are small, since a faster machine will be able to process proportionally more splits over the course of the job than a slower machine. </p></li>
<li><p>Even if the machines are identical, failed processes or other jobs running concurrently make load balancing desirable, and the quality of the load balancing increases as the splits become more fine-grained.</p></li>
<li><p>if splits are too small, then the overhead of managing the splits and of map task creation begins to dominate the total job execution time. For most jobs, a good split size tends to be the size of an HDFS block, 64 MB by default.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-65" style="background:;">
  <hgroup>
    <h2>What is data locality optimization?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Hadoop does its best to run the map task on a node where the input data resides in HDFS. This is called the data locality optimization. </p></li>
<li><p>It should now be clear why the optimal split size is the same as the block size: it is the largest size of input that can be guaranteed to be stored on a single node. </p></li>
<li><p>If the split spanned two blocks, it would be unlikely that any HDFS node stored both blocks, so some of the split would have to be transferred across the network to the node running the map task, which is clearly less efficient than running the whole map task using local data.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-66" style="background:;">
  <hgroup>
    <h2>What is hadoop hardware?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>The term commodity hardware is often used to describe Hadoop hardware requirements.</li>
</ul>

<p>-commodity refers to mid-level rack servers with dual sockets, as much error-correcting RAM as is affordable, and SATA drives optimized for RAID storage.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-67" style="background:;">
  <hgroup>
    <h2>What are site and default configuration xml files?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The site XML files (those with site in their filenames) will grow as you start customizing your Hadoop cluster, and it can quickly become challenging to keep track of what changes youâ€™ve made, and how they relate to the default configuration values.</p></li>
<li><p>default files</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-68" style="background:;">
  <hgroup>
    <h2>What does core-site.xml refer to?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>core-site.xml Contains system-level Hadoop configuration items, such as the HDFS URL, the Hadoop temporary directory, and script locations for rack-aware Hadoop clusters. </p></li>
<li><p>Settings in this file override the settings in core-default.xml. The default settings can be seen at <a href="http://hadoop.apache.org/common/docs/r1.0.0/core-default.html">http://hadoop.apache.org/common/docs/r1.0.0/core-default.html</a>.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-69" style="background:;">
  <hgroup>
    <h2>What does hdfs-site.xml refer to?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>hdfs-site.xml Contains HDFS settings such as the default file replication count, the block size, and whether permissions are enforced. To view the default settings you can look at <a href="http://hadoop.apache.org/common/docs/r1.0.0/hdfs-default.html">http://hadoop.apache.org/common/docs/r1.0.0/hdfs-default.html</a>. </li>
<li>Settings in this file override the settings in hdfs-default.xml. </li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-70" style="background:;">
  <hgroup>
    <h2>What does mapred-site.xml refer to?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>mapred-site.xml HDFS settings such as the default number of reduce tasks, default min/max task memory sizes, and speculative execution are all set here. </p></li>
<li><p>To view the default settings you can look at <a href="http://hadoop.apache.org/common/docs/r1.0.0/mapreddefault.html">http://hadoop.apache.org/common/docs/r1.0.0/mapreddefault.html</a>. </p></li>
<li><p>Settings in this file override the settings in mapred-default.xml.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-71" style="background:;">
  <hgroup>
    <h2>What does the term master and slave refer to?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Contains a list of hosts that are Hadoop masters. This name is misleading and should have been called secondary-masters. </p></li>
<li><p>When you start Hadoop itâ€™ll launch NameNode and JobTracker on the local host from which you issued the start command, and then SSH to all the nodes in this file to launch the SecondaryNameNode.</p></li>
<li><p>slaves Contains a list of hosts that are Hadoop slaves. When you start Hadoop it will SSH to each host in this file and launch the DataNode and TaskTracker daemons.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-72" style="background:;">
  <hgroup>
    <h2>What is YARN?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Apache YARN (Yet Another Resource Negotiator) is Hadoop&#39;s cluster resource management system. </p></li>
<li><p>YARN was introduced in Hadoop 2 to improve the MapReduce implementation,</p></li>
<li><p>YARN provides APIs for requesting and working with cluster resources, but these APIs are not typically used directly by user code. </p></li>
<li><p>Instead, users write to higher-level APIs provided by distributed computing frameworks, which themselves are built on YARN and hide the resource management details from the user.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-73" style="background:;">
  <hgroup>
    <h2>What is the need for Yarn?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Hadoop  is  one  of  the  widely-adopted  cluster  computing  frameworks  for  processing  of  the  Big  Data.  </p></li>
<li><p>Although Hadoop  arguably  has  become  the  standard  solution  for  managing  Big  Data,  it  is  not  free  from  limitations.  MapReduce  lias  reached  scalability  limit  of  4000  nodes,  Another  limitation  is  Hadoop&#39;s  inability  to  perform  fine-grained  resource  sharing  between multiple  computation  frameworks.  </p></li>
<li><p>To  solve  these  limitations,  the  open  source  community  proposed  the  next  generation  MapReduce  called  YARN  (Yet  Another  Resource  Negotiator),Computer  scientists  and  engineers  are  trying  hard  to  eliminate  these  limitations  and  improve  Hadoop.  </p></li>
<li><p>YARN  eliminates  scalability  limitation  of  the  first  generation  MapReduce  paradigm</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-74" style="background:;">
  <hgroup>
    <h2>What is the core service of Yarn?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>YARN provides its core services via two types of long-running daemon: a resource manager (one per cluster) to manage the use of resources across the cluster, and node managers running on all the nodes in the cluster to launch and monitor containers. </p></li>
<li><p>A container executes an application-specific process with a constrained set of resources
(memory, CPU, and so on). Depending on how YARN is configured, a container may be a Unix process.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-75" style="background:;">
  <hgroup>
    <h2>What are the components of Yarn ?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>The YARN system has the following components:</li>
<li>Global Resource Manager</li>
<li>Node Manager</li>
<li>Application-specific Application Master</li>
<li>Scheduler</li>
<li>Container</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-76" style="background:;">
  <hgroup>
    <h2>What is a container in Yarn? (1/4)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The container is a computational unit in the YARN framework. It is a subsystem in which a unit of work occurs. Or, in the language of MapReduce v1, it is a component in which the equivalent of a task executes. The relationship between a container and a node is this: one node runs several containers, but a container cannot cross a node boundary.</p></li>
<li><p>A container is a set of allocated system resources. Currently only two types of system resources are supported: - CPU core -  Memory in MB</p></li>
<li><p>The container comprising the resources will execute on a certain node, so implicit in a container is the notion of the &quot;resource name&quot; that is the name of the rack and the node on which the container runs. When a container is requested, it is requested on a specific node. </p></li>
<li><p>Thus, a container is a right conferred upon an application to use a specific number of CPU cores and a specific amount of memory on a specific host.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-77" style="background:;">
  <hgroup>
    <h2>What is a container in Yarn? (2/4)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Any job or application (single job or DAG of jobs) will essentially run in one or more containers. </p></li>
<li><p>All  containers  in  YARN including AMs  are  described  by  a container  launch  context (CLC).  </p></li>
<li><p>This record includes a map of environment variables, dependencies  stored  in  remotely  accessible  storage,  security tokens,  payloads  for  NM  services,  and  the  command necessary to create the process. </p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-78" style="background:;">
  <hgroup>
    <h2>What is a container in Yarn? (3/4)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>After validating the authenticity of the lease, the NM configures the environment for the container, including initializing its monitoring  subsystem  with  the  resource  constraints  specified in the lease. </p></li>
<li><p>To launch the container, the NM copies all the necessary dependencies- data files, executables, tarballs- to local storage. If required, the CLC also includes credentials to authenticate the download. </p></li>
</ul>

<p>Dependencies may be shared between containers in an application, between containers launched by the same tenant, and even between tenants, as specified in the CLC. </p>

<p>The NM eventually garbage collects dependencies not in use by running containers.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-79" style="background:;">
  <hgroup>
    <h2>What is a container in Yarn? (4/4)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The NM will also kill containers as directed by the RM or the AM. Containers may be killed when the RM reports its owning application as  completed, when the scheduler decides to evict it for another tenant, or when the  NM  detects  that  the  container  exceeded  the  limits of its lease. </p></li>
<li><p>AMs may request containers to be killed when the corresponding work isn&#39;t needed any more. </p></li>
<li><p>Whenever a container exits, the NM will clean up its working directory in local storage.  </p></li>
<li><p>When an application completes, all resources owned by its containers are discarded on all nodes, including any of its processes still running in the cluster.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-80" style="background:;">
  <hgroup>
    <h2>What is a Node Manager in Yarn? (1/5)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The YARN framework entity that is ultimately responsible for physically allocating a container is called a Node Manager.</p></li>
<li><p>A Node Manager runs on a single node in the cluster, and each node in the cluster runs its own Node Manager. </p></li>
<li><p>It is a slave service: it takes requests from another component called the Resource Manager and allocates containers to applications. </p></li>
<li><p>It is also responsible for monitoring and reporting usage metrics to the Resource Manager. </p></li>
<li><p>Together with the Resource Manager, the Node Manager forms the framework responsible for managing the resource allocation on the Hadoop cluster. </p></li>
<li><p>While the Resource Manager is a global component, the Node Manager is a per-node agent responsible for managing the health of individual nodes in the Hadoop cluster. </p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-81" style="background:;">
  <hgroup>
    <h2>What are the tasks of Node Manager in Yarn?(2/5)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The NodeManager is the &quot;worker&quot; daemon in YARN. </p></li>
<li><p>It authenticates container leases, manages containers&#39; dependencies, monitors their execution, and provides a set of services to containers. </p></li>
<li><p>Operators configure it to report memory, CPU, and other resources available at this node and allocated for YARN. </p></li>
<li><p>After registering with the RM, the NM heartbeats its status and receives instructions.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-82" style="background:;">
  <hgroup>
    <h2>What are the tasks of Node Manager in Yarn?(3/5)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Tasks of the Node Manager include the following:</p></li>
<li><p>Receives requests from the Resource Manager and allocates containers on behalf of the job.</p></li>
<li><p>Exchanges messages with the Resource Manager to ensure the smooth functioning of the overall cluster. </p></li>
<li><p>The Resource Manager keeps track of global health based on reports received from each Node Manager, which is delegated the task of monitoring and managing its own health.</p></li>
<li><p>Manages the life cycle of each launched container.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-83" style="background:;">
  <hgroup>
    <h2>What are the tasks of Node Manager in Yarn?(4/5)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Logs management on each node.</p></li>
<li><p>Executes auxiliary services exploited by various YARN applications. </p></li>
<li><p>When a node starts up, it registers with the Resource Manager and tells the Resource Manager how many of
resources (eventually allocated to form containers) are available. </p></li>
<li><p>During runtime, this information is constantly updated as the Node Manager and Resource Manager work together to ensure a fully functional and optimally utilized cluster.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-84" style="background:;">
  <hgroup>
    <h2>What are the tasks of Node Manager in Yarn?(5/5)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>NM periodically monitors the health of the physical  node.  </p></li>
<li><p>It  monitors  any  issues  with  the  local  disks, and runs an admin configured script frequently that in
turn  can  point  to  any  hardware/software  issues.  </p></li>
<li><p>When such an issue is discovered, NM changes its state to be unhealthy  and  reports  RM  about  the  same  which  then makes a scheduler specific decision of killing the containers and/or  stopping future  allocations on this  node till the health issue is addressed</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-85" style="background:;">
  <hgroup>
    <h2>What is a Resource Manager in Yarn?(1/5)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The Resource Manager is primarily a scheduler: it arbitrates resources among competing applications to ensure optimal cluster utilization. </p></li>
<li><p>The Resource Manager has a pluggable scheduler that is responsible for allocating resources to the various running applications, subject to familiar constraints of capacities and queues. </p></li>
<li><p>The actual task of creating, provisioning, and monitoring resources is delegated to the per-node Node Manager. This separation of concerns enables the Resource Manager to scale much more than the traditional JobScheduler.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-86" style="background:;">
  <hgroup>
    <h2>What is a Resource Manager in Yarn?(2/5)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The RM runs as a daemon on a dedicated machine, and acts as the central authority arbitrating resources among various competing applications in the cluster. </p></li>
<li><p>Given this central and global view of the cluster resources, it can enforce rich, familiar properties such as fairness, capacity,  and  locality across  tenants.</p></li>
<li><p>The resource manager has two primary modules: Scheduler and ApplicationManager.  </p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-87" style="background:;">
  <hgroup>
    <h2>What is a Resource Manager in Yarn?(3/5)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>For every incoming job the ApplicationManager starts an ApplicationMaster on one of the slave nodes.  </p></li>
<li><p>The Application Master makes resource requests to the resource manager and is also responsible for monitoring the status of the job.   </p></li>
<li><p>Jobs are divided into tasks and for every task the scheduler assigns a container upon the request from the corresponding ApplicationMaster.  </p></li>
<li><p>A container specifies the  node  to  run  the  task  on  and  a  fixed  amount  of  resources (memory and CPU cores).  </p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-88" style="background:;">
  <hgroup>
    <h2>What is a Resource Manager in Yarn? (4/5)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>YARN supports allocating containers based on the available resources (as of now just based on memory) on the nodes, but it has no mechanism to determine the actual resource requirements of a job.</p></li>
<li><p>Depending on the application demand, scheduling priorities, and resource availability, the RM dynamically allocates leases- called containers to applications to run on particular nodes.
The container is a logical bundle of resources bound to a particular node. </p></li>
<li><p>In order to enforce and track such assignments, the RM interacts with a special system daemon running on each node called the NodeManager (NM).</p></li>
<li><p>Communications between RM and NMs are heartbeat based for scalability. NMs are responsible for monitoring resource availability, reporting faults, and container lifecycle  management  (e.g.,  starting,  killing).<br>
The  RM assembles its global view from these snapshots of NM state.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-89" style="background:;">
  <hgroup>
    <h2>How does Resource Manager manage jobs? (5/5)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Jobs  are  submitted  to  the  RM  via  a  public  submission protocol and go through an admission control phase during which security credentials are validated and various operational and administrative checks are performed.  </p></li>
<li><p>Accepted  jobs  are  passed  to  the  scheduler  to  be run. Once the scheduler has enough resources, the application is moved from accepted to running state. </p></li>
<li><p>Aside from  internal  bookkeeping,  this  involves  allocating  a container for the AM and spawning it on a node in the cluster. </p></li>
<li><p>A record of accepted applications is written to persistent storage and recovered in case of RM restart or
failure.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-90" style="background:;">
  <hgroup>
    <h2>What is Application Master in Yarn? (1/3)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The Application Master is the key differentiator between the older MapReduce v1 framework and YARN. </p></li>
<li><p>The Application Master is an instance of a framework-specific library. </p></li>
<li><p>It negotiates resources from the Resource Manager and works with the Node Manager to acquire those resources and execute its tasks. </p></li>
<li><p>The Application Master is the component that negotiates resource containers from the Resource Manager.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-91" style="background:;">
  <hgroup>
    <h2>What are the benefits of Application Master? (2/3)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The key benefits the Application Master brings to the YARN framework are these:- Improved scalability - A more generic framework</p></li>
<li><p>In MapReduce v1, the responsibility for managing the task failover rested with the JobTracker. </p></li>
<li><p>The JobTracker also had the responsibility for allocating resources to jobs. </p></li>
<li><p>Scalability is improved in v2 because the Resource Manager (the replacement for JobTracker) is now  responsible only for scheduling. </p></li>
<li><p>The task of managing the jobs or application rests with the Application Master. If a task fails, the Application Master will negotiate resources from the Resource Manager and attempt to re-execute the task.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-92" style="background:;">
  <hgroup>
    <h2>What is an application with Respect to AM? (3/3)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>An application may be a static set of processes, a logical description of work, or even a long-running service. </p></li>
<li><p>The ApplicationMaster is the process that coordinates the application&#39;s execution in the cluster, but it itself is run in the cluster just like any other container. </p></li>
<li><p>A component of the RM negotiates for the container to spawn this bootstrap process.</p></li>
<li><p>The AM periodically heartbeats to the RM to affirm its liveness and to update the record of its demand. After building a model of its requirements, the AM encodes its preferences and constraints in a heartbeat message to the RM. </p></li>
<li><p>In response to subsequent heartbeats, the AM will receive a container lease on bundles of resources bound to a particular node in the cluster. Based on the containers it receives from the RM, the AM may update
its execution plan to accommodate perceived abundance or scarcity.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-93" style="background:;">
  <hgroup>
    <h2>What are the interfaces exposed by ResoureManager?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>The ResourceManager exposes two public interfaces towards:  1)clients  submitting  applications,  and  2)  ApplicationMaster(s) dynamically negotiating access to resources,  and  one  internal  interface  towards  NodeManagers for cluster monitoring and resource access management.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-94" style="background:;">
  <hgroup>
    <h2>What are schedulers?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The Scheduler is responsible for a llocating the resources among running applications. </p></li>
<li><p>The ApplicationsManager accepts job-submissions,   negotiating   the   first   container   for   executing   the   application   and   provides   the   service   for   restarting   the  ApplicationMaster container on failure.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-95" style="background:;">
  <hgroup>
    <h2>What are schedulers in YARN?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Schedulers play a critical role in achieving desired performance levels in a Hadoop system. </p></li>
<li><p>However, the heterogeneity level of each factor potentially has a significant effect on performance. It is critical to select a scheduling algorithm by considering the Hadoop factors, and the desired performance level. </p></li>
<li><p>A scheduling algorithm which performs well in one Hadoop system, may not work well for a system that differs in these factors</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-96" style="background:;">
  <hgroup>
    <h2>What are the schedulers available in Yarn?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Three schedulers are available in YARN: the FIFO, Capacity, and Fair Schedulers. </li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-97" style="background:;">
  <hgroup>
    <h2>What is FIFO scheduler ? (1/4)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The FIFO Scheduler places applications in a queue and runs them in the order of submission
(first in, first out). </p></li>
<li><p>Requests for the first application in the queue are allocated first; once its requests have been satisfied, the next application in the queue is served, and so on.</p></li>
<li><p>The FIFO Scheduler has the merit of being simple to understand and not needing any configuration, but it&#39;s not suitable for shared clusters. </p></li>
<li><p>Large applications will use all the resources in a cluster, so each application has to wait its turn.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-98" style="background:;">
  <hgroup>
    <h2>What is FairShare Scheduler? (1/4)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Fair scheduling is a method of allocating resources to jobs such that all jobs on an average get an equal share of resources over time. </p></li>
<li><p>The  Fair scheduler is intended to give a fair share of cluster capacity over time. </p></li>
<li><p>If only one job is running, it has the  privilege to access all the capacity of cluster. </p></li>
<li><p>As users submit more jobs, free task slots are shared among each user in a manner it  gives  fair share  of the  cluster. </p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-99" style="background:;">
  <hgroup>
    <h2>What is FairShare Scheduler? Contd(2/4)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The  fair scheduler is  fair enough  with both  shorter and longer jobs in the  way  that  it lets shorter 
jobs to finish in a reasonable time  while not starving longer jobs.  This also allows for multiple users to share cluster in an easy manner. </p></li>
<li><p>Fair sharing can also work with jobs assigned priorities. </p></li>
<li><p>The  Fair  Scheduler  arranges  jobs  into  pools  and  allocates  resources  fairly  among  these  pools.  </p></li>
<li><p>Each  user  is  assigned  a  pool  by default  which  allows  for  equal  sharing  of  the  cluster.  </p></li>
<li><p>Inside  each  pool  either  FIFO  or  fair  sharing  scheduling  would  have  been employed. Within each pool, the default model is to share the pool across all jobs submitted to that pool. </p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-100" style="background:;">
  <hgroup>
    <h2>What is FairShare Scheduler? Contd(3/4)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Therefore, if the  cluster is  split  into  pools  for  two  users  say  user  A  and  user  B,  each  of  whom  submit  three  jobs,  the  cluster  will  execute  all  six  jobs  in  parallel. </p></li>
<li><p>Suppose a pool has not received its fair share for some time period, then the scheduler optionally supports preemption of jobs in other pools. </p></li>
<li><p>The Scheduler will be allowed to kill tasks in pools running over capacity so that slots can be given to the  pools running under capacity. </p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-101" style="background:;">
  <hgroup>
    <h2>What is FairShare Scheduler? Contd(4/4)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>In order to guarantee that not to starve &quot;production&quot; jobs, the pre-emption can be  used while still  allowing the Hadoop cluster to be used by research and experimental jobs</p></li>
<li><p>Queues can have weights, which are used in the fair share calculation</p></li>
<li><p>The Fair Scheduler is configured using an allocation file named fair-scheduler.xml that
is loaded from the classpath</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-102" style="background:;">
  <hgroup>
    <h2>What is capacity Scheduler? (1/3)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The Capacity Scheduler allows sharing of a Hadoop cluster along organizational lines, whereby each organization is allocated a certain capacity of the overall cluster. </p></li>
<li><p>Capacity  scheduler has  been  designed  specifically  for  those  enviromnents  where  there  is  need  for  fair  sharing  of  computation resources among large  number of users. </p></li>
<li><p>It takes a  slightly different approach to  multiuser  scheduling.  </p></li>
<li><p>Capacity Scheduler configuration file, called capacity-scheduler.xml</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-103" style="background:;">
  <hgroup>
    <h2>What is capacity Scheduler? (2/3)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>A  cluster  is  made up of a number of queues, which may be hierarchical and each queue has a capacity allocated to it. Within each queue jobs  are  scheduled  using  prioritized  FIFO  scheduling.  </p></li>
<li><p>In  effect,  the  capacity  scheduler  allows  users  or  organization  to  simulate  a  separate MapReduce cluster with FIFO scheduling for each user or organization.</p></li>
<li><p>Each organization is set up with a dedicated queue that is configured to use a given fraction of the cluster capacity. </p></li>
<li><p>Queues may be further divided in hierarchical fashion, allowing each organization to share its cluster allowance between different groups of users within the organization. Within a queue, applications are scheduled using FIFO scheduling.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-104" style="background:;">
  <hgroup>
    <h2>What is queue elasticity? (3/3)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>if there is more than one job in the queue and there are idle resources available, then the Capacity Scheduler may allocate the spare resources to jobs in the queue, even if that causes the queue&#39;s capacity to be exceeded. This behavior is known as queue elasticity.</p></li>
<li><p>In normal operation, the Capacity Scheduler does not preempt containers by forcibly killing them, so if a queue is under capacity due to lack of demand, and then demand increases, the queue will only return to capacity as resources are released from other queues as containers complete. </p></li>
<li><p>It is possible to mitigate this by configuring queues with a maximum capacity so that they don&#39;t eat into other queues&#39; capacities too much. This is at the cost of queue elasticity, of course, so a reasonable tradeoff should be found by trial and error.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-105" style="background:;">
  <hgroup>
    <h2>What are Queues?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Queues are implementations to hold the job for execution.</p></li>
<li><p>Queues can have different scheduling policies. </p></li>
<li><p>The default policy for queues can be set in the top-level defaultQueueSchedulingPolicy element; if it is omitted, fair scheduling is used. </p></li>
<li><p>Despite its name, the Fair Scheduler also supports a FIFO (fifo) policy on queues, as well as Dominant Resource Fairness (drf),</p></li>
<li><p>The policy for a particular queue can be overridden using the schedulingPolicy element
for that queue.</p></li>
<li><p>queues can be configured with minimum and maximum resources, and a maximum number of running applications</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-106" style="background:;">
  <hgroup>
    <h2>What is queue placement?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The Fair Scheduler uses a rules-based system to determine which queue an application is placed in</p></li>
<li><p>Unless the queue is explicitly specified, the user&#39;s name is used for the queue</p></li>
<li><p>to set this policy without using an allocation file, by setting yarn.scheduler.fair.user-as-default-queue to false so that applications will be placed in the default queue rather than a per-user queue.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-107" style="background:;">
  <hgroup>
    <h2>What is preemption?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>When a job is submitted to an empty queue on a busy cluster, the job cannot start until resources free up from jobs that are already running on the cluster. To make the time taken for a job to start more predictable, the Fair Scheduler supports preemption.</p></li>
<li><p>Preemption allows the scheduler to kill containers for queues that are running with more than their fair share of resources so that the resources can be allocated to a queue that is under its fair share.</p></li>
<li><p>Preemption is enabled globally by setting yarn.scheduler.fair.preemption to true. There are two relevant preemption timeout settings: one for minimum share and one for fair share, both specified in seconds.</p></li>
<li><p>If a queue waits for as long as its minimum share preemption timeout without receiving its minimum guaranteed share, then the scheduler may preempt other containers. </p></li>
<li><p>The default timeout is set for all queues via the defaultMinSharePreemptionTimeout toplevel element in the allocation file, and on a per-queue basis by setting the minShare PreemptionTimeout element for a queue.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-108" style="background:;">
  <hgroup>
    <h2>What is Delay scheduling?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>On a busy cluster, if an application requests a particular node, there is a good chance that other containers are running on it at the time of the request. </p></li>
<li><p>The obvious course of action is to immediately loosen the locality requirement and allocate a container on the same rack. </p></li>
<li><p>However, it has been observed in practice that waiting a short time (no more than a few seconds) can dramatically increase the chances of being allocated a container on the requested node, and therefore increase the efficiency of the cluster. </p></li>
<li><p>This feature is called delay scheduling, and it is supported by both the Capacity Scheduler and the Fair Scheduler.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-109" style="background:;">
  <hgroup>
    <h2>What is job initialization in Yarn? (1/2)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>When the resource manager receives a call to its submitApplication() method, it hands off the request to the YARN scheduler. </p></li>
<li><p>The scheduler allocates a container, and the resource manager then launches the application master&#39;s process there, under the node manager&#39;s management</p></li>
<li><p>The application master for MapReduce jobs is a Java application whose main class is MRAppMaster. </p></li>
<li><p>It initializes the job by creating a number of bookkeeping objects to keep track of the job&#39;s progress, as it will receive progress and completion reports from the tasks</p></li>
<li><p>Next, it retrieves the input splits computed in the client from the shared filesystem</p></li>
<li><p>It then creates a map task object for each split, as well as a number of reduce task objects determined by the mapreduce.job.reduces property (set by the setNumReduceTasks() method on Job). Tasks are given IDs at this point.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-110" style="background:;">
  <hgroup>
    <h2>What is job initialization in Yarn? (2/2)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The application master must decide how to run the tasks that make up the MapReduce job. </p></li>
<li><p>If the job is small, the application master may choose to run the tasks in the same JVM as itself. </p></li>
<li><p>This happens when it judges that the overhead of allocating and running tasks in new containers outweighs the gain to be had in running them in parallel, compared to running them sequentially on one node. Such a job is said to be uberized, or run as an uber task.</p></li>
<li><p>Uber tasks must be enabled explicitly (for an individual job, or across the cluster) by setting mapreduce.job.ubertask.enable to true.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-111" style="background:;">
  <hgroup>
    <h2>What qualifies as a small job?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>By default, a small job is one that has less than 10 mappers, only one reducer, and an input size that is less than the size of one HDFS block</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-112" style="background:;">
  <hgroup>
    <h2>What is task assignment? (1/2)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>If the job does not qualify for running as an uber task, then the application master requests containers for all the map and reduce tasks in the job from the resource manager</p></li>
<li><p>Requests for map tasks are made first and with a higher priority than those for reduce tasks, since all the map tasks must complete before the sort phase of the reduce can start</p></li>
<li><p>Requests for reduce tasks are not made until 5% of map tasks have completed</p></li>
<li><p>Reduce tasks can run anywhere in the cluster, but requests for map tasks have data locality constraints that the scheduler tries to honor. </p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-113" style="background:;">
  <hgroup>
    <h2>What is task assignment? (2/2)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>In the optimal case, the task is data local-that is, running on the same node that the split resides on. Alternatively, the task may be rack local: on the same rack, but not the same node, as the split. </p></li>
<li><p>Some tasks are neither data local nor rack local and retrieve their data from a different rack than the one they are running on. For a particular job run, you can determine the number of tasks that ran at each locality level by looking at the job&#39;s counters.</p></li>
<li><p>Requests also specify memory requirements and CPUs for tasks. By default, each map and reduce task is allocated 1,024 MB of memory and one virtual core. The values are configurable on a per-job basis via the following properties: mapreduce.map.memory.mb, mapreduce.reduce.memory.mb, mapreduce.map.cpu
.vcores and mapreduce.reduce.cpu.vcores.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-114" style="background:;">
  <hgroup>
    <h2>What is Task execution? (1/2)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Once a task has been assigned resources for a container on a particular node by the resource manager&#39;s scheduler, the application master starts the container by contacting the node manager. </p></li>
<li><p>The task is executed by a Java application whose main class is YarnChild. Before it can run the task, it localizes the resources that the task needs, including the job configuration and JAR file, and any files from the distributed cache</p></li>
<li><p>Finally, it runs the map or reduce task</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-115" style="background:;">
  <hgroup>
    <h2>What is Task execution? (2/2)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Each task can perform setup and commit actions, which are run in the same JVM as the task itself and are determined by the OutputCommitter for the job</p></li>
<li><p>For file-based jobs, the commit action moves the task output from a temporary location to its final location. </p></li>
<li><p>The commit protocol ensures that when speculative execution is enabled, only one of the duplicate tasks is committed and the other is aborted</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-116" style="background:;">
  <hgroup>
    <h2>What does streaming task do  ?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Streaming runs special map and reduce tasks for the purpose of launching the usersupplied executable and communicating with it.</p></li>
<li><p>The Streaming task communicates with the process (which may be written in any language) using standard input and output streams. </p></li>
<li><p>During execution of the task, the Java process passes input key-value pairs to the external process, which runs it through the user-defined map or reduce function and passes the output key-value pairs back to the Java process. </p></li>
<li><p>From the node manager&#39;s point of view, it is as if the child process ran the map or reduce code itself.</p></li>
</ul>

<hr>

<h2>What are the operations which constitutes progress?</h2>

<ul>
<li>All of the following operations constitute progress:</li>
<li> Reading an input record (in a mapper or reducer)</li>
<li> Writing an output record (in a mapper or reducer)</li>
<li> Setting the status description (via Reporter&#39;s or TaskAttemptContext&#39;s setStatus() method)</li>
<li> Incrementing a counter (using Reporter&#39;s incrCounter() method or Counter&#39;s increment() method)</li>
<li> Calling Reporter&#39;s or TaskAttemptContext&#39;s progress() method</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-117" style="background:;">
  <hgroup>
    <h2>What is progress status update?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>MapReduce jobs are long-running batch jobs, taking anything from tens of seconds to hours to run. Because this can be a significant length of time, it&#39;s important for the user to get feedback on how the job is progressing. </p></li>
<li><p>A job and each of its tasks have a status, which includes such things as the state of the job or task (e.g., running, successfully completed, failed), the progress of maps and reduces, the values of the job&#39;s counters, and a status message or description (which may be set by user code).</p></li>
<li><p>When a task is running, it keeps track of its progress (i.e., the proportion of the task
completed). </p></li>
<li><p>For map tasks, this is the proportion of the input that has been processed.</p></li>
<li><p>For reduce tasks, it&#39;s a little more complex, but the system can still estimate the proportion of the reduce input processed. It does this by dividing the total progress into three parts, corresponding to the three phases of the shuffle.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-118" style="background:;">
  <hgroup>
    <h2>What is the use of Umblical interface?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>As the map or reduce task runs, the child process communicates with its parent application master through the umbilical interface. </p></li>
<li><p>The task reports its progress and status (including counters) back to its application master, which has an aggregate view of the job, every three seconds over the umbilical interface.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-119" style="background:;">
  <hgroup>
    <h2>What is job completion?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>When the application master receives a notification that the last task for a job is complete, it changes the status for the job to &quot;successful.&quot; Then, when the Job polls for status, it learns that the job has completed successfully, so it prints a message to tell the user and then returns from the waitForCompletion() method. Job statistics and counters are printed to the console at this point.</p></li>
<li><p>The application master also sends an HTTP job notification if it is configured to do so.</p></li>
<li><p>This can be configured by clients wishing to receive callbacks, via the mapreduce.job.end-notification.url property.</p></li>
<li><p>Finally, on job completion, the application master and the task containers clean up their working state (so intermediate output is deleted), and the OutputCommitter&#39;s commit Job() method is called. Job information is archived by the job history server to enable later interrogation by users if desired.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-120" style="background:;">
  <hgroup>
    <h2>What is task failure? (1/6)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The most common occurrence of task failure is when user code in the map or reduce task throws a runtime exception. </p></li>
<li><p>If this happens, the task JVM reports the error back to its parent application master before it exits. </p></li>
<li><p>The error ultimately makes it into the user logs. </p></li>
<li><p>The application master marks the task attempt as failed, and frees up the container so its resources are available for another task.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-121" style="background:;">
  <hgroup>
    <h2>What is task failure? (2/6)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>For Streaming tasks, if the Streaming process exits with a nonzero exit code, it is marked as failed. This behavior is governed by the stream.non.zero.exit.is.failure property (the default is true).</p></li>
<li><p>Another failure mode is the sudden exit of the task JVM-perhaps there is a JVM bug that causes the JVM to exit for a particular set of circumstances exposed by the MapReduce user code. </p></li>
<li><p>In this case, the node manager notices that the process has exited and informs the application master so it can mark the attempt as failed. Hanging tasks are dealt with differently. </p></li>
<li><p>The application master notices that it hasn&#39;t received a progress update for a while and proceeds to mark the task as failed. </p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-122" style="background:;">
  <hgroup>
    <h2>What is task failure? (3/6)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The task JVM process will be killed automatically after this period. </p></li>
<li><p>The timeout period after which tasks are considered failed is normally 10 minutes and can be configured on a per-job basis (or a cluster basis) by setting the mapreduce.task.timeout property to a value in milliseconds.</p></li>
<li><p>When the application master is notified of a task attempt that has failed, it will reschedule
execution of the task. </p></li>
<li><p>The application master will try to avoid rescheduling the task on a node manager where it has previously failed. </p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-123" style="background:;">
  <hgroup>
    <h2>What is task failure? (4/6)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Furthermore, if a task fails four times, it will not be retried again. This value is configurable. </p></li>
<li><p>The maximum number of attempts to run a task is controlled by the mapreduce.map.maxattempts property for map tasks and mapreduce.reduce.maxattempts for reduce tasks. </p></li>
<li><p>By default, if any task fails four times (or whatever the maximum number of attempts is configured to), the whole job fails.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-124" style="background:;">
  <hgroup>
    <h2>What is task failure? (5/6)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>For some applications, it is undesirable to abort the job if a few tasks fail, as it may be possible to use the results of the job despite some failures. </p></li>
<li><p>In this case, the maximum percentage of tasks that are allowed to fail without triggering job failure can be set for the job. </p></li>
<li><p>Map tasks and reduce tasks are controlled independently, using the mapreduce.map.failures.maxpercent and mapreduce.reduce.failures.maxper cent properties.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-125" style="background:;">
  <hgroup>
    <h2>What is task failure? (6/6)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>A task attempt may also be killed, which is different from it failing. A task attempt may
be killed because it is a speculative duplicate, or because the node manager it was running on
failed and the application master marked all the task attempts running on it as killed.</p></li>
<li><p>Killed task attempts do not count against the number of attempts to run the task (as set
by mapreduce.map.maxattempts and mapreduce.reduce.maxattempts), because it wasn&#39;t the task&#39;s fault that an attempt was killed.</p></li>
<li><p>Users may also kill or fail task attempts using the web UI or the command line (type
mapred job to see the options). Jobs may be killed by the same mechanisms.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-126" style="background:;">
  <hgroup>
    <h2>Get in touch</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Reach out to your instructor</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-127" style="background:;">
  <article data-timings="">
    
  </article>
  <!-- Presenter Notes -->
</slide>

    <slide class="backdrop"></slide>
  </slides>
  <div class="pagination pagination-small" id='io2012-ptoc' style="display:none;">
    <ul>
      <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=1 title='Let Us Discuss HDFS'>
         1
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=2 title='What is distributed Systems?'>
         2
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=3 title='What is a filesystem in Hadoop?'>
         3
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=4 title='What is HDFS?'>
         4
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=5 title='What  is Streaming data access?'>
         5
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=6 title='What is Commodity hardware ?'>
         6
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=7 title='What is the need for HDFS ?'>
         7
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=8 title='What are HDFS architectural tradeoffs'>
         8
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=9 title='What is a disk block? (1/2)'>
         9
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=10 title='What is a disk block? (2/2)'>
         10
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=11 title='Why HDFS blocks are large?'>
         11
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=12 title='Why are the benefits of DFS? (1/2)'>
         12
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=13 title='Why are the benefits of DFS? (2/2)'>
         13
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=14 title='What is HDFS data integrity? (1/2)'>
         14
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=15 title='What is HDFS data integrity? (2/2)'>
         15
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=16 title='What are the Hadoop daemons?(1/2)'>
         16
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=17 title='What are the Hadoop daemons?(2/2)'>
         17
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=18 title='What is name node?(1/2)'>
         18
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=19 title='What is name node?(2/2)'>
         19
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=20 title='What is a Data Node?'>
         20
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=21 title='What is a Job tracker ?'>
         21
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=22 title='What is task tracker? (1/3)'>
         22
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=23 title='What is task tracker? (2/3)'>
         23
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=24 title='What is task tracker? (3/3)'>
         24
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=25 title='What does data node store?'>
         25
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=26 title='How does name node store meta data?'>
         26
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=27 title='What is File system Namespace ?'>
         27
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=28 title='What is a federation?'>
         28
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=29 title='What is HDFS Federation?'>
         29
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=30 title='What are the benefits of federation?'>
         30
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=31 title='What is a block pool?'>
         31
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=32 title='What is a namespace volume ?'>
         32
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=33 title='How do you access Federated HDFS cluster?'>
         33
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=34 title='What is HDFS high availability?'>
         34
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=35 title='What is checkpoint?'>
         35
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=36 title='What is checkpoint?'>
         36
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=37 title='What are the architectural changes in 2.X for High availability/'>
         37
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=38 title='What is failover controller?'>
         38
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=39 title='What is fencing?'>
         39
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=40 title='How is the file stored in Data Node?'>
         40
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=41 title='What are file permissions in HDFS? (1/2)'>
         41
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=42 title='What are file permissions in HDFS? (2/2) contd..'>
         42
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=43 title='What are the data formats in HDFS?'>
         43
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=44 title='How does a client access the filesystem?'>
         44
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=45 title='How can HDFS be accessed?'>
         45
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=46 title='What are Hadoop interfaces?'>
         46
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=47 title='What are Hadoop interfaces? contd..'>
         47
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=48 title='What is data ingress and egress in hadoop?'>
         48
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=49 title='What are Hadoop specific file types ?'>
         49
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=50 title='What is a sequentialFile ?'>
         50
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=51 title='What is the structure of sequence file? (1/2)'>
         51
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=52 title='What is the structure of sequence file? (2/2)'>
         52
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=53 title='NA'>
         53
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=54 title='What is a MapFile'>
         54
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=55 title='What is Map File varients?'>
         55
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=56 title='What are row oriented file formats?'>
         56
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=57 title='What are column oriented file formats?'>
         57
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=58 title='What is file compression?'>
         58
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=59 title='What is file compression in Hadoop?'>
         59
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=60 title='What are file compression tools in Hadoop?'>
         60
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=61 title='What are the methods of compression codec?'>
         61
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=62 title='What is MapReduce?'>
         62
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=63 title='What is the responsibility of MapReduce framework?'>
         63
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=64 title='What are haoop splits?'>
         64
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=65 title='What is data locality optimization?'>
         65
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=66 title='What is hadoop hardware?'>
         66
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=67 title='What are site and default configuration xml files?'>
         67
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=68 title='What does core-site.xml refer to?'>
         68
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=69 title='What does hdfs-site.xml refer to?'>
         69
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=70 title='What does mapred-site.xml refer to?'>
         70
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=71 title='What does the term master and slave refer to?'>
         71
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=72 title='What is YARN?'>
         72
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=73 title='What is the need for Yarn?'>
         73
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=74 title='What is the core service of Yarn?'>
         74
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=75 title='What are the components of Yarn ?'>
         75
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=76 title='What is a container in Yarn? (1/4)'>
         76
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=77 title='What is a container in Yarn? (2/4)'>
         77
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=78 title='What is a container in Yarn? (3/4)'>
         78
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=79 title='What is a container in Yarn? (4/4)'>
         79
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=80 title='What is a Node Manager in Yarn? (1/5)'>
         80
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=81 title='What are the tasks of Node Manager in Yarn?(2/5)'>
         81
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=82 title='What are the tasks of Node Manager in Yarn?(3/5)'>
         82
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=83 title='What are the tasks of Node Manager in Yarn?(4/5)'>
         83
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=84 title='What are the tasks of Node Manager in Yarn?(5/5)'>
         84
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=85 title='What is a Resource Manager in Yarn?(1/5)'>
         85
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=86 title='What is a Resource Manager in Yarn?(2/5)'>
         86
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=87 title='What is a Resource Manager in Yarn?(3/5)'>
         87
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=88 title='What is a Resource Manager in Yarn? (4/5)'>
         88
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=89 title='How does Resource Manager manage jobs? (5/5)'>
         89
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=90 title='What is Application Master in Yarn? (1/3)'>
         90
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=91 title='What are the benefits of Application Master? (2/3)'>
         91
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=92 title='What is an application with Respect to AM? (3/3)'>
         92
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=93 title='What are the interfaces exposed by ResoureManager?'>
         93
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=94 title='What are schedulers?'>
         94
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=95 title='What are schedulers in YARN?'>
         95
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=96 title='What are the schedulers available in Yarn?'>
         96
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=97 title='What is FIFO scheduler ? (1/4)'>
         97
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=98 title='What is FairShare Scheduler? (1/4)'>
         98
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=99 title='What is FairShare Scheduler? Contd(2/4)'>
         99
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=100 title='What is FairShare Scheduler? Contd(3/4)'>
         100
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=101 title='What is FairShare Scheduler? Contd(4/4)'>
         101
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=102 title='What is capacity Scheduler? (1/3)'>
         102
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=103 title='What is capacity Scheduler? (2/3)'>
         103
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=104 title='What is queue elasticity? (3/3)'>
         104
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=105 title='What are Queues?'>
         105
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=106 title='What is queue placement?'>
         106
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=107 title='What is preemption?'>
         107
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=108 title='What is Delay scheduling?'>
         108
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=109 title='What is job initialization in Yarn? (1/2)'>
         109
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=110 title='What is job initialization in Yarn? (2/2)'>
         110
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=111 title='What qualifies as a small job?'>
         111
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=112 title='What is task assignment? (1/2)'>
         112
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=113 title='What is task assignment? (2/2)'>
         113
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=114 title='What is Task execution? (1/2)'>
         114
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=115 title='What is Task execution? (2/2)'>
         115
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=116 title='What does streaming task do  ?'>
         116
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=117 title='What is progress status update?'>
         117
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=118 title='What is the use of Umblical interface?'>
         118
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=119 title='What is job completion?'>
         119
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=120 title='What is task failure? (1/6)'>
         120
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=121 title='What is task failure? (2/6)'>
         121
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=122 title='What is task failure? (3/6)'>
         122
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=123 title='What is task failure? (4/6)'>
         123
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=124 title='What is task failure? (5/6)'>
         124
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=125 title='What is task failure? (6/6)'>
         125
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=126 title='Get in touch'>
         126
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=127 title='NA'>
         127
      </a>
    </li>
  </ul>
  </div>  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
  <!-- Load Javascripts for Widgets -->
  
  <!-- LOAD HIGHLIGHTER JS FILES -->
  <script src="libraries/highlighters/highlight.js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <!-- DONE LOADING HIGHLIGHTER JS FILES -->
   
  </html>
